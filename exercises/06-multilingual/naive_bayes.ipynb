{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T16:57:53.937820Z",
     "start_time": "2020-05-21T16:57:52.634431Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from utils import load_data, print_data_stats, subset_data\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the dataset\n",
    "- What is Sentiment Analysis?\n",
    "    - \"I like this movie\" --> positive\n",
    "    - \"I hate this movie\" --> negative\n",
    "- [ABSA multilingual SA dataset](http://alt.qcri.org/semeval2016/task5/)\n",
    "- What kind of preprocessing can you think of from the examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T16:57:54.731139Z",
     "start_time": "2020-05-21T16:57:54.693076Z"
    }
   },
   "outputs": [],
   "source": [
    "LANGS = [\"ar\",\"en\",\"es\",\"ru\",\"zh\"]\n",
    "LANGS_MAPPING = {\"en\":\"english\",\"es\":\"spanish\",\"ru\":\"russian\",\"ar\":\"arabic\",\"zh\":\"chinese\"}\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T07:51:13.457405Z",
     "start_time": "2020-05-21T07:51:13.429689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#train</th>\n",
       "      <th>#test</th>\n",
       "      <th>train-pos%</th>\n",
       "      <th>test-pos%</th>\n",
       "      <th>sample</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>1608</td>\n",
       "      <td>555</td>\n",
       "      <td>0.673507</td>\n",
       "      <td>0.718919</td>\n",
       "      <td>I'm saving up for my next visit.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>1535</td>\n",
       "      <td>650</td>\n",
       "      <td>0.716612</td>\n",
       "      <td>0.678462</td>\n",
       "      <td>Calidad-precio muy bien.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>2663</td>\n",
       "      <td>865</td>\n",
       "      <td>0.755914</td>\n",
       "      <td>0.677457</td>\n",
       "      <td>Удачи вам и процветания!!!</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>4438</td>\n",
       "      <td>1145</td>\n",
       "      <td>0.605453</td>\n",
       "      <td>0.567686</td>\n",
       "      <td>فريق العمل ودود ومستعد لإسعاد الضيوف</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>1333</td>\n",
       "      <td>529</td>\n",
       "      <td>0.568642</td>\n",
       "      <td>0.586011</td>\n",
       "      <td>但质量，</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    #train  #test  train-pos%  test-pos%  \\\n",
       "en    1608    555    0.673507   0.718919   \n",
       "es    1535    650    0.716612   0.678462   \n",
       "ru    2663    865    0.755914   0.677457   \n",
       "ar    4438   1145    0.605453   0.567686   \n",
       "zh    1333    529    0.568642   0.586011   \n",
       "\n",
       "                                  sample label  \n",
       "en      I'm saving up for my next visit.   pos  \n",
       "es              Calidad-precio muy bien.   pos  \n",
       "ru            Удачи вам и процветания!!!   pos  \n",
       "ar  فريق العمل ودود ومستعد لإسعاد الضيوف   pos  \n",
       "zh                                  但质量，   neg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_data_stats(data, max_len=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- re-sample the data to make all languages to have the same number of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T15:26:42.865935Z",
     "start_time": "2020-05-18T15:26:42.828476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#train</th>\n",
       "      <th>#test</th>\n",
       "      <th>train-pos%</th>\n",
       "      <th>test-pos%</th>\n",
       "      <th>sample</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>1333</td>\n",
       "      <td>555</td>\n",
       "      <td>0.669917</td>\n",
       "      <td>0.718919</td>\n",
       "      <td>Overpriced and not tasty</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>1333</td>\n",
       "      <td>650</td>\n",
       "      <td>0.717179</td>\n",
       "      <td>0.678462</td>\n",
       "      <td>El asado muy bueno.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>1333</td>\n",
       "      <td>865</td>\n",
       "      <td>0.757689</td>\n",
       "      <td>0.677457</td>\n",
       "      <td>Общее впечатление приятное.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>1333</td>\n",
       "      <td>1145</td>\n",
       "      <td>0.609152</td>\n",
       "      <td>0.567686</td>\n",
       "      <td>أسوأ فندق في باريس</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>1333</td>\n",
       "      <td>529</td>\n",
       "      <td>0.568642</td>\n",
       "      <td>0.586011</td>\n",
       "      <td>手感也很好。</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    #train  #test  train-pos%  test-pos%                       sample label\n",
       "en    1333    555    0.669917   0.718919    Overpriced and not tasty    neg\n",
       "es    1333    650    0.717179   0.678462         El asado muy bueno.    pos\n",
       "ru    1333    865    0.757689   0.677457  Общее впечатление приятное.   pos\n",
       "ar    1333   1145    0.609152   0.567686           أسوأ فندق في باريس   neg\n",
       "zh    1333    529    0.568642   0.586011                     手感也很好。     pos"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_sampled = subset_data(data)\n",
    "print_data_stats(data_sampled,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: build your own dataset (20 for train, 5 for test)\n",
    "- try to use similar words as much as possible\n",
    "- try to make some word overlaps between examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T07:53:32.276540Z",
     "start_time": "2020-05-21T07:53:32.260881Z"
    }
   },
   "outputs": [],
   "source": [
    "# NEW_LANG = \"?\"\n",
    "# train_pos_sents = [\"I like this movie\",\"Ihe nkiri a masịrị m\", \"Ninapenda sinema hii\"]\n",
    "# train_neg_sents = [\"I have this movie\"]\n",
    "# test_pos_sents = [\"I enjoyed the movie\"]\n",
    "# test_neg_sents = [\"Never watch it\"]\n",
    "\n",
    "# data[NEW_LANG] = {}\n",
    "# data[NEW_LANG][\"train\"] = [(sent,\"pos\") for sent in train_pos_sents] + [(sent,\"neg\") for sent in train_neg_sents]\n",
    "# data[NEW_LANG][\"test\"] = [(sent,\"pos\") for sent in train_pos_sents] + [(sent,\"neg\") for sent in train_neg_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load stemmers, word_tokenizers, stopword_filters\n",
    "- **stemming/lemmatization**: reducing inflected (or sometimes derived) words to their word stem\n",
    "- **word segmentation (tokenization)**: dividing a string of written language into its component words\n",
    "- **stopwords**: a set of commonly used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T07:58:17.912972Z",
     "start_time": "2020-05-21T07:58:17.872548Z"
    }
   },
   "outputs": [],
   "source": [
    "import Stemmer\n",
    "import stopwordsiso as stopwordsiso\n",
    "import jieba\n",
    "from pyarabic import araby \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import stopwordsiso\n",
    "\n",
    "class MultiStopword:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.stopwords = {}\n",
    "        for lang in [\"en\",\"es\",\"ar\",\"ru\"]:\n",
    "            self.stopwords[lang] = set(stopwords.words(LANGS_MAPPING[lang]))\n",
    "        for lang in [\"zh\"]:\n",
    "            self.stopwords[lang] = stopwordsiso.stopwords(lang) \n",
    "        \n",
    "        # # TODO\n",
    "        # self.stopwords[NEW_LANG] = set([\"\"])\n",
    "            \n",
    "    def is_stopword(self, word,lang):\n",
    "        \n",
    "        if lang in self.stopwords:\n",
    "            return (word in self.stopwords[lang])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class MultiWordSegmenter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.tokenizer = {}\n",
    "        self.tokenizer[\"ru\"] = ToktokTokenizer()\n",
    "\n",
    "    def segment(self, text, lang):\n",
    "        \n",
    "        if lang in [\"en\",\"es\"]:\n",
    "            return word_tokenize(text, language=LANGS_MAPPING[lang])\n",
    "        elif lang == \"zh\":\n",
    "            return jieba.cut(text)\n",
    "        elif lang == \"ru\":\n",
    "            return self.tokenizer[\"ru\"].tokenize(text)\n",
    "        elif lang == \"ar\":\n",
    "            return araby.tokenize(text)\n",
    "        \n",
    "        ## TODO\n",
    "        # elif lang == NEW_LANG:\n",
    "        #    return word_tokenize(text)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class MultiWordStemmers:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.stemmers = {}\n",
    "        self.stemmers[\"en\"] = Stemmer.Stemmer('english')\n",
    "        self.stemmers[\"ar\"] = Stemmer.Stemmer('arabic')\n",
    "        self.stemmers[\"ru\"] = Stemmer.Stemmer('russian')\n",
    "        self.stemmers[\"es\"] = Stemmer.Stemmer('spanish')\n",
    "\n",
    "    def stem(self, word, lang):\n",
    "        \n",
    "        if lang in self.stemmers:\n",
    "            return self.stemmers[lang].stemWord(word)\n",
    "        elif lang == \"zh\":\n",
    "            return word\n",
    "        \n",
    "        # elif lang == NEW_LANG:\n",
    "        #    ## TODO\n",
    "        #    return word\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "stopword_checkers = MultiStopword()\n",
    "word_segmenters = MultiWordSegmenter()\n",
    "stemmers = MultiWordStemmers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T15:27:16.550831Z",
     "start_time": "2020-05-18T15:27:16.542494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend\n",
      "friend\n",
      "friend\n"
     ]
    }
   ],
   "source": [
    "print(stemmers.stem(\"friend\",\"en\"))\n",
    "print(stemmers.stem(\"friends\",\"en\"))\n",
    "print(stemmers.stem(\"friended\",\"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T15:32:44.028259Z",
     "start_time": "2020-05-18T15:32:44.020115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "долж\n",
      "должн\n",
      "должн\n",
      "должн\n"
     ]
    }
   ],
   "source": [
    "# russian verbs for MUST\n",
    "print(stemmers.stem(\"должен\",\"ru\")) # Male\n",
    "print(stemmers.stem(\"должна\",\"ru\")) # Female\n",
    "print(stemmers.stem(\"должно\",\"ru\")) # Neutral\n",
    "print(stemmers.stem(\"должны\",\"ru\")) # Plural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T07:57:16.517005Z",
     "start_time": "2020-05-21T07:57:16.504323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mr.Brown', False)\n",
      "('measur', False)\n",
      "('the', True)\n",
      "('cat', False)\n",
      "('this', True)\n",
      "('morn', False)\n",
      "(',', False)\n",
      "('and', True)\n",
      "('it', True)\n",
      "('was', True)\n",
      "('14.5', False)\n",
      "('pound', False)\n",
      "('!', False)\n",
      "Mr.Brown measured cat morning , 14.5 pounds !\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_example(sentence, lang):\n",
    "    \n",
    "    print(\"\\n\".join([str((stemmers.stem(w,lang), stopword_checkers.is_stopword(w,lang))) for w in word_segmenters.segment(sentence,lang)]))\n",
    "    print(\" \".join([w for w in word_segmenters.segment(sentence,lang) if not stopword_checkers.is_stopword(w,lang)]))\n",
    "\n",
    "ex_sentence = \"Mr.Brown measured the cat this morning, and it was 14.5 pounds!\"\n",
    "preprocessing_example(ex_sentence, \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T07:55:30.612149Z",
     "start_time": "2020-05-21T07:55:30.606646Z"
    }
   },
   "source": [
    "# Activity: add stemmers, word_tokenizers, stopwords for your language\n",
    "- find and edit `## TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T07:11:04.482933Z",
     "start_time": "2020-05-21T07:11:04.474637Z"
    }
   },
   "outputs": [],
   "source": [
    "# # and test out!\n",
    "# ex_sentence = \"your sentence!\"\n",
    "# preprocessing_example(ex_sentence, NEW_LANG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2: reduce the number of features (# of unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Love the scene first off- the place has a character and nice light to it..very fortunate, location wise.',\n",
       " 'pos')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the shap of the dataset for \"en\"\n",
    "\n",
    "data_sampled['en']['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T07:11:14.887051Z",
     "start_time": "2020-05-21T07:11:14.570872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 2988\n",
      "es 3581\n",
      "ru 4465\n",
      "ar 7964\n",
      "zh 1438\n"
     ]
    }
   ],
   "source": [
    "baseline = {}\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), lowercase=False)\n",
    "\n",
    "for lang in data_sampled.keys():\n",
    "    sentences_train, y_train = zip(*data_sampled[lang][\"train\"])\n",
    "    vectorizer.fit(sentences_train)\n",
    "    num_unigram = len(vectorizer.get_feature_names_out())\n",
    "    baseline[lang] = num_unigram\n",
    "    print(lang, num_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T17:53:36.375859Z",
     "start_time": "2020-05-13T17:53:35.511894Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar: 7778 (186↓)\n",
      "en: 2512 (476↓)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ehhho\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es: 3085 (496↓)\n",
      "ru: 3930 (535↓)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.998 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zh: 1479 (-41↓)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(sentence_list:list, lang:str) -> list:\n",
    "    return [preprocess_sentence(sentence, lang) for sentence in sentence_list]\n",
    "\n",
    "\n",
    "def preprocess_sentence(text:str, lang:str) -> str:\n",
    "    text = text.lower()\n",
    "    \n",
    "    ## TODO\n",
    "    # words = text.split()\n",
    "    words = word_segmenters.segment(text, lang)\n",
    "    words = [w for w in words if not stopword_checkers.is_stopword(w, lang)]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), lowercase=False)\n",
    "for lang in LANGS:\n",
    "    sentences_train, y_train = zip(*data_sampled[lang][\"train\"])\n",
    "    sentences_train = preprocess(sentences_train, lang)\n",
    "    vectorizer.fit(sentences_train)\n",
    "    num_unigram = len(vectorizer.get_feature_names_out())\n",
    "    print(f\"{lang}: {num_unigram:<5}({baseline[lang]-num_unigram}\\u2193)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T07:47:37.366345Z",
     "start_time": "2020-05-21T07:47:37.339195Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence_list:list, lang:str, bool_lowercase=False, bool_segment=False, bool_stem=False, filter_stopwords=False) -> list:\n",
    "    return [preprocess_sentence(sentence, lang, bool_lowercase, bool_segment, bool_stem, filter_stopwords) for sentence in sentence_list]\n",
    "\n",
    "def preprocess_sentence(text:str, lang:str, bool_lowercase, bool_segment, bool_stem, filter_stopwords) -> str:\n",
    "    if bool_lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    if bool_segment:\n",
    "        words = word_segmenters.segment(text, lang)\n",
    "    else:\n",
    "        words = text.split()\n",
    "\n",
    "    if bool_stem:\n",
    "        words = [stemmers.stem(w, lang) for w in words]\n",
    "    \n",
    "    if filter_stopwords:\n",
    "        words = [w for w in words if not stopword_checkers.is_stopword(w, lang)]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def train_and_evaluate_nb(data:dict, lang:str, max_feat=100) -> float:\n",
    "    sentences_train, y_train = zip(*data[lang][\"train\"])\n",
    "    sentences_test, y_test = zip(*data[lang][\"test\"])\n",
    "    \n",
    "    sentences_train, sentences_test = preprocess(sentences_train, lang), preprocess(sentences_test, lang)\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=max_feat, lowercase=False)\n",
    "    x_train = vectorizer.fit_transform(sentences_train)\n",
    "    x_test = vectorizer.transform(sentences_test)    \n",
    "    \n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train, y_train)\n",
    "    acc = model.score(x_test, y_test)\n",
    "    print(f\"{lang}: {acc:.2f}\")\n",
    "    return {\"model\":model, \"vectorizer\":vectorizer}\n",
    "\n",
    "def predict(models, lang, sents):\n",
    "    model, vectorizer = models[lang][\"model\"], models[lang][\"vectorizer\"]\n",
    "    if type(sents) == str:\n",
    "        sents = [sents]\n",
    "    sents = preprocess(sents, lang)\n",
    "    x = vectorizer.transform(sents)\n",
    "    pred = model.predict(x)\n",
    "    print(list(zip(sents,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T07:49:13.125301Z",
     "start_time": "2020-05-21T07:49:11.024168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: 0.74\n",
      "es: 0.76\n",
      "ru: 0.73\n",
      "ar: 0.73\n",
      "zh: 0.59\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for lang in data.keys():\n",
    "    models[lang] = train_and_evaluate_nb(data, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T08:01:28.589927Z",
     "start_time": "2020-05-21T08:01:28.581344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t1\n",
      "  (0, 45)\t1\n",
      "  (0, 94)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 45)\t1\n",
      "  (1, 55)\t1\n",
      "  (1, 94)\t1\n",
      "[('will watch it again', 'pos'), ('will not watch it again', 'neg')]\n"
     ]
    }
   ],
   "source": [
    "ex_sents = [\"will watch it again\",\"will not watch it again\"]\n",
    "print(models[\"en\"][\"vectorizer\"].transform(ex_sents))\n",
    "predict(models,\"en\",ex_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3: Fill out the following table\n",
    "- change boolean arguments in `preprocess()`\n",
    "\n",
    "|                          \t| Ar \t| En \t| Es \t| Ru \t| Zh \t|\n",
    "|--------------------------\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|\n",
    "| Baseline                 \t|  0.73 |  0.74 |  0.76 |  0.73 |  0.59 |\n",
    "| All                      \t|  0.76 |  0.74 |  0.78 |  0.74 |  0.67 |\n",
    "| All - segmentation       \t|  0.75 |  0.75 |  0.78 |  0.74 |  0.59 |\n",
    "| All - stemmer            \t|  0.69 |  0.74 |  0.73 |  0.73 |  0.67 |\n",
    "| All - stopword_filtering \t|  0.79 |  0.76 |  0.82 |  0.75 |  0.68 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 4: Explain your own observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- observation 1: Stop words are important for sentiement classification in general.\n",
    "- observation 2: Stemming affects the quality of the results except for Chinese."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0d65d99df97b6cfb344dc3baa8c6231a78ce1274520b01ad9b5bd5ec718ab994"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
